{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNsudZcbOuF5oUU0o6vODK7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solidbridge/Auto-GPT/blob/master/finalproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itjZN8a0Y2zV"
      },
      "outputs": [],
      "source": [
        "# Enhanced package installation with version control\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from typing import Dict, List, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def install_packages(packages: Dict[str, str]) -> None:\n",
        "    \"\"\"Install packages with specific versions\"\"\"\n",
        "    for package, version in packages.items():\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"{package}{version}\"])\n",
        "            print(f\"âœ… Successfully installed {package}{version}\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"âŒ Failed to install {package}: {e}\")\n",
        "\n",
        "# Core packages with compatible versions\n",
        "REQUIRED_PACKAGES = {\n",
        "    \"langchain\": \">=0.1.0\",\n",
        "    \"langchain-openai\": \">=0.1.0\",\n",
        "    \"langchain-experimental\": \">=0.0.50\",\n",
        "    \"langchain-community\": \">=0.0.20\",\n",
        "    \"openai\": \">=1.0.0\",\n",
        "    \"pandas\": \">=2.0.0\",\n",
        "    \"numpy\": \">=1.24.0\",\n",
        "    \"matplotlib\": \">=3.7.0\",\n",
        "    \"seaborn\": \">=0.12.0\",\n",
        "    \"scikit-learn\": \">=1.3.0\",\n",
        "    \"plotly\": \">=5.15.0\",\n",
        "    \"streamlit\": \">=1.25.0\",\n",
        "    \"faiss-cpu\": \">=1.7.0\",\n",
        "    \"pypdf\": \">=3.15.0\",\n",
        "    \"python-dotenv\": \">=1.0.0\",\n",
        "    \"sqlalchemy\": \">=2.0.0\",\n",
        "    \"chromadb\": \">=0.4.0\"\n",
        "}\n",
        "\n",
        "# Install packages\n",
        "install_packages(REQUIRED_PACKAGES)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core data science libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Statistical analysis\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# LangChain imports (fixed)\n",
        "try:\n",
        "    from langchain_experimental.agents.agent_toolkits.pandas.base import create_pandas_dataframe_agent\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    from langchain.agents import create_sql_agent, AgentType\n",
        "    from langchain_community.utilities import SQLDatabase\n",
        "    from langchain_community.document_loaders import PyPDFLoader\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    from langchain_community.vectorstores import FAISS, Chroma\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "    from langchain.chains import RetrievalQA\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    print(\"âœ… All LangChain imports successful\")\n",
        "except ImportError as e:\n",
        "    print(f\"âš ï¸ Import warning: {e}\")\n",
        "    print(\"Installing missing dependencies...\")\n",
        "\n",
        "# Database and file handling\n",
        "import sqlite3\n",
        "from sqlalchemy import create_engine\n",
        "import json\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "# Environment setup\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n"
      ],
      "metadata": {
        "id": "Z9lc3_S_aIAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataScienceConfig:\n",
        "    \"\"\"Configuration management for the data science assistant\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.setup_logging()\n",
        "        self.setup_api_keys()\n",
        "        self.setup_model_config()\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Setup logging configuration\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler('data_science_assistant.log'),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def setup_api_keys(self):\n",
        "        \"\"\"Setup API keys with environment variables\"\"\"\n",
        "        # Use os.environ.get for safer access and fallback\n",
        "        # Remove hardcoded key and rely on environment variable set by Secrets Manager\n",
        "        self.openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
        "        if not self.openai_api_key:\n",
        "            self.logger.warning(\"OpenAI API key not found in environment variables. Please add it to Colab Secrets Manager with the name OPENAI_API_KEY.\")\n",
        "\n",
        "        # Set environment variable for consistent access (if not already set)\n",
        "        if 'OPENAI_API_KEY' not in os.environ:\n",
        "             os.environ['OPENAI_API_KEY'] = self.openai_api_key\n",
        "\n",
        "\n",
        "    def setup_model_config(self):\n",
        "        \"\"\"Setup model configurations\"\"\"\n",
        "        self.model_configs = {\n",
        "            'gpt-4': {\n",
        "                'model_name': 'gpt-4',\n",
        "                'temperature': 0.1,\n",
        "                'max_tokens': 2000\n",
        "            },\n",
        "            'gpt-3.5-turbo': {\n",
        "                'model_name': 'gpt-3.5-turbo',\n",
        "                'temperature': 0.1,\n",
        "                'max_tokens': 1500\n",
        "            }\n",
        "        }\n",
        "\n",
        "# Initialize configuration\n",
        "config = DataScienceConfig()"
      ],
      "metadata": {
        "id": "sNJIDV78aIOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMManager:\n",
        "    \"\"\"Enhanced LLM management with multiple model support\"\"\"\n",
        "\n",
        "    def __init__(self, config: DataScienceConfig):\n",
        "        self.config = config\n",
        "        self.models = {}\n",
        "        self.setup_models()\n",
        "\n",
        "    def setup_models(self):\n",
        "        \"\"\"Setup multiple LLM models\"\"\"\n",
        "        for model_name, model_config in self.config.model_configs.items():\n",
        "            try:\n",
        "                # Pass the API key explicitly\n",
        "                self.models[model_name] = ChatOpenAI(**model_config, openai_api_key=self.config.openai_api_key)\n",
        "                self.config.logger.info(f\"âœ… {model_name} model initialized successfully\")\n",
        "            except Exception as e:\n",
        "                self.config.logger.error(f\"âŒ Failed to initialize {model_name}: {e}\")\n",
        "\n",
        "    def get_model(self, model_name: str = 'gpt-4') -> ChatOpenAI:\n",
        "        \"\"\"Get a specific model\"\"\"\n",
        "        return self.models.get(model_name, self.models['gpt-3.5-turbo'])\n",
        "\n",
        "    def test_model_connection(self, model_name: str = 'gpt-4') -> bool:\n",
        "        \"\"\"Test model connection\"\"\"\n",
        "        try:\n",
        "            model = self.get_model(model_name)\n",
        "            response = model.invoke(\"Hello, please respond with 'Connection successful'\")\n",
        "            return \"successful\" in response.content.lower()\n",
        "        except Exception as e:\n",
        "            self.config.logger.error(f\"Model connection test failed: {e}\")\n",
        "            return False\n",
        "\n",
        "# Initialize LLM manager\n",
        "llm_manager = LLMManager(config)\n",
        "\n",
        "# Test connection\n",
        "if llm_manager.test_model_connection():\n",
        "    print(\"âœ… LLM connection successful\")\n",
        "else:\n",
        "    print(\"âŒ LLM connection failed\")"
      ],
      "metadata": {
        "id": "_1-hvyD0aIWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataProcessor:\n",
        "    \"\"\"Enhanced data processing with multiple data sources\"\"\"\n",
        "\n",
        "    def __init__(self, llm_manager: LLMManager):\n",
        "        self.llm_manager = llm_manager\n",
        "        self.data = {}\n",
        "        self.agents = {}\n",
        "\n",
        "    def load_csv_data(self, file_path: str, name: str = 'default') -> pd.DataFrame:\n",
        "        \"\"\"Load CSV data with enhanced error handling\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            self.data[name] = df\n",
        "            config.logger.info(f\"âœ… Loaded {name} dataset: {df.shape}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            config.logger.error(f\"âŒ Failed to load {file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def create_enhanced_agent(self, df: pd.DataFrame, name: str = 'default') -> None:\n",
        "        \"\"\"Create enhanced pandas agent with safety measures\"\"\"\n",
        "        try:\n",
        "            agent = create_pandas_dataframe_agent(\n",
        "                llm=self.llm_manager.get_model(),\n",
        "                df=df,\n",
        "                verbose=True,\n",
        "                allow_dangerous_code=True,  # Required for pandas agent\n",
        "                agent_type=AgentType.OPENAI_FUNCTIONS,\n",
        "                handle_parsing_errors=True,\n",
        "                max_iterations=5\n",
        "            )\n",
        "            self.agents[name] = agent\n",
        "            config.logger.info(f\"âœ… Created agent for {name}\")\n",
        "        except Exception as e:\n",
        "            config.logger.error(f\"âŒ Failed to create agent for {name}: {e}\")\n",
        "\n",
        "    def analyze_dataset(self, df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Comprehensive dataset analysis\"\"\"\n",
        "        analysis = {\n",
        "            'shape': df.shape,\n",
        "            'columns': list(df.columns),\n",
        "            'dtypes': df.dtypes.to_dict(),\n",
        "            'missing_values': df.isnull().sum().to_dict(),\n",
        "            'numerical_summary': df.describe().to_dict(),\n",
        "            'categorical_summary': {}\n",
        "        }\n",
        "\n",
        "        # Categorical analysis\n",
        "        for col in df.select_dtypes(include=['object']).columns:\n",
        "            analysis['categorical_summary'][col] = {\n",
        "                'unique_count': df[col].nunique(),\n",
        "                'top_values': df[col].value_counts().head().to_dict()\n",
        "            }\n",
        "\n",
        "        return analysis\n",
        "\n",
        "# Initialize data processor\n",
        "data_processor = DataProcessor(llm_manager)\n",
        "\n",
        "# Sample data creation for demonstration\n",
        "def create_sample_dataset():\n",
        "    \"\"\"Create a comprehensive sample dataset\"\"\"\n",
        "    np.random.seed(42)\n",
        "    n_samples = 1000\n",
        "\n",
        "    data = {\n",
        "        'age': np.random.normal(45, 15, n_samples).astype(int),\n",
        "        'income': np.random.lognormal(10, 0.5, n_samples),\n",
        "        'education_years': np.random.randint(8, 20, n_samples),\n",
        "        'experience': np.random.randint(0, 30, n_samples),\n",
        "        'job_satisfaction': np.random.randint(1, 11, n_samples),\n",
        "        'department': np.random.choice(['IT', 'Sales', 'HR', 'Finance', 'Marketing'], n_samples),\n",
        "        'performance_score': np.random.normal(75, 15, n_samples),\n",
        "        'training_hours': np.random.exponential(20, n_samples),\n",
        "        'promotion': np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Add some correlations\n",
        "    df.loc[df['performance_score'] > 85, 'promotion'] = 1\n",
        "    df.loc[df['job_satisfaction'] > 8, 'performance_score'] += 10\n",
        "\n",
        "    return df\n",
        "\n",
        "# Create and load sample data\n",
        "sample_df = create_sample_dataset()\n",
        "data_processor.data['employee_data'] = sample_df\n",
        "data_processor.create_enhanced_agent(sample_df, 'employee_data')\n"
      ],
      "metadata": {
        "id": "DpkZLAEFaIZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedVisualizer:\n",
        "    \"\"\"Advanced visualization suite with multiple chart types\"\"\"\n",
        "\n",
        "    def __init__(self, data_processor: DataProcessor):\n",
        "        self.data_processor = data_processor\n",
        "        self.setup_style()\n",
        "\n",
        "    def setup_style(self):\n",
        "        \"\"\"Setup visualization style\"\"\"\n",
        "        plt.style.use('seaborn-v0_8')\n",
        "        sns.set_palette(\"husl\")\n",
        "\n",
        "    def create_comprehensive_eda(self, df: pd.DataFrame, target_col: str = None) -> None:\n",
        "        \"\"\"Create comprehensive exploratory data analysis\"\"\"\n",
        "\n",
        "        # 1. Dataset Overview\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
        "        fig.suptitle('Comprehensive Exploratory Data Analysis', fontsize=16, y=1.02)\n",
        "\n",
        "        # Missing values heatmap\n",
        "        sns.heatmap(df.isnull(), cbar=True, ax=axes[0,0], cmap='viridis')\n",
        "        axes[0,0].set_title('Missing Values Heatmap')\n",
        "\n",
        "        # Correlation matrix\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        if len(numeric_cols) > 1:\n",
        "            corr_matrix = df[numeric_cols].corr()\n",
        "            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[0,1])\n",
        "            axes[0,1].set_title('Correlation Matrix')\n",
        "\n",
        "        # Distribution of numerical variables\n",
        "        if len(numeric_cols) > 0:\n",
        "            df[numeric_cols].hist(bins=30, ax=axes[1,0], alpha=0.7)\n",
        "            axes[1,0].set_title('Distribution of Numerical Variables')\n",
        "\n",
        "        # Categorical variables\n",
        "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "        if len(categorical_cols) > 0:\n",
        "            cat_col = categorical_cols[0]\n",
        "            df[cat_col].value_counts().plot(kind='bar', ax=axes[1,1])\n",
        "            axes[1,1].set_title(f'Distribution of {cat_col}')\n",
        "            axes[1,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def create_interactive_dashboard(self, df: pd.DataFrame) -> None:\n",
        "        \"\"\"Create interactive Plotly dashboard\"\"\"\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        if len(numeric_cols) >= 2:\n",
        "            # Interactive scatter plot\n",
        "            fig1 = px.scatter(df, x=numeric_cols[0], y=numeric_cols[1],\n",
        "                             color=df.columns[-1] if df.columns[-1] in df.select_dtypes(include=['object']).columns else None,\n",
        "                             title=\"Interactive Scatter Plot\",\n",
        "                             hover_data=[col for col in df.columns[:5]])\n",
        "            fig1.show()\n",
        "\n",
        "            # Interactive correlation heatmap\n",
        "            corr_matrix = df[numeric_cols].corr()\n",
        "            fig2 = px.imshow(corr_matrix, text_auto=True, aspect=\"auto\",\n",
        "                           title=\"Interactive Correlation Heatmap\")\n",
        "            fig2.show()\n",
        "\n",
        "    def create_advanced_statistical_plots(self, df: pd.DataFrame, target_col: str) -> None:\n",
        "        \"\"\"Create advanced statistical visualizations\"\"\"\n",
        "\n",
        "        if target_col not in df.columns:\n",
        "            print(f\"Target column '{target_col}' not found\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(24, 16))\n",
        "        fig.suptitle(f'Advanced Statistical Analysis for {target_col}', fontsize=16)\n",
        "\n",
        "        # Box plots for numerical variables\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        numeric_cols = [col for col in numeric_cols if col != target_col]\n",
        "\n",
        "        if len(numeric_cols) >= 2:\n",
        "            sns.boxplot(data=df, x=target_col, y=numeric_cols[0], ax=axes[0,0])\n",
        "            axes[0,0].set_title(f'{numeric_cols[0]} by {target_col}')\n",
        "\n",
        "            sns.violinplot(data=df, x=target_col, y=numeric_cols[1], ax=axes[0,1])\n",
        "            axes[0,1].set_title(f'{numeric_cols[1]} Distribution by {target_col}')\n",
        "\n",
        "        # Density plots\n",
        "        if len(numeric_cols) >= 1:\n",
        "            for i, category in enumerate(df[target_col].unique()[:3]):\n",
        "                subset = df[df[target_col] == category]\n",
        "                sns.kdeplot(data=subset[numeric_cols[0]], ax=axes[0,2], label=f'{category}')\n",
        "            axes[0,2].set_title(f'{numeric_cols[0]} Density by {target_col}')\n",
        "            axes[0,2].legend()\n",
        "\n",
        "        # Statistical test results visualization\n",
        "        if df[target_col].dtype in ['object', 'category'] and len(numeric_cols) >= 1:\n",
        "            # Perform ANOVA or t-test\n",
        "            groups = [df[df[target_col] == cat][numeric_cols[0]].dropna() for cat in df[target_col].unique()]\n",
        "            if len(groups) == 2:\n",
        "                stat, p_value = stats.ttest_ind(groups[0], groups[1])\n",
        "                test_name = \"T-Test\"\n",
        "            else:\n",
        "                stat, p_value = stats.f_oneway(*groups)\n",
        "                test_name = \"ANOVA\"\n",
        "\n",
        "            axes[1,0].text(0.1, 0.5, f'{test_name}\\nStatistic: {stat:.4f}\\nP-value: {p_value:.4f}',\n",
        "                          fontsize=12, transform=axes[1,0].transAxes)\n",
        "            axes[1,0].set_title('Statistical Test Results')\n",
        "            axes[1,0].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Initialize visualizer\n",
        "visualizer = AdvancedVisualizer(data_processor)\n",
        "\n",
        "# Create comprehensive EDA for sample data\n",
        "visualizer.create_comprehensive_eda(sample_df, 'promotion')\n",
        "visualizer.create_interactive_dashboard(sample_df)\n",
        "visualizer.create_advanced_statistical_plots(sample_df, 'promotion')\n"
      ],
      "metadata": {
        "id": "pPs-DuwdaIb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPipeline:\n",
        "    \"\"\"Comprehensive machine learning pipeline\"\"\"\n",
        "\n",
        "    def __init__(self, llm_manager: LLMManager):\n",
        "        self.llm_manager = llm_manager\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "        self.scalers = {}\n",
        "\n",
        "    def prepare_data(self, df: pd.DataFrame, target_col: str, test_size: float = 0.2):\n",
        "        \"\"\"Enhanced data preparation\"\"\"\n",
        "        # Separate features and target\n",
        "        X = df.drop(columns=[target_col])\n",
        "        y = df[target_col]\n",
        "\n",
        "        # Handle categorical variables\n",
        "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "        X_processed = X.copy()\n",
        "\n",
        "        for col in categorical_cols:\n",
        "            le = LabelEncoder()\n",
        "            X_processed[col] = le.fit_transform(X[col].astype(str))\n",
        "\n",
        "        # Handle missing values\n",
        "        X_processed = X_processed.fillna(X_processed.mean())\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_processed, y, test_size=test_size, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        self.scalers[target_col] = scaler\n",
        "\n",
        "        return X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled\n",
        "\n",
        "    def train_multiple_models(self, X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled):\n",
        "        \"\"\"Train multiple ML models\"\"\"\n",
        "\n",
        "        models = {\n",
        "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "            'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "            'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
        "            'Neural Network': MLPClassifier(random_state=42, max_iter=1000)\n",
        "        }\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for name, model in models.items():\n",
        "            try:\n",
        "                # Use scaled data for models that benefit from it\n",
        "                if name in ['Logistic Regression', 'Neural Network']:\n",
        "                    model.fit(X_train_scaled, y_train)\n",
        "                    y_pred = model.predict(X_test_scaled)\n",
        "                    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "                else:\n",
        "                    model.fit(X_train, y_train)\n",
        "                    y_pred = model.predict(X_test)\n",
        "                    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "                # Calculate metrics\n",
        "                accuracy = model.score(X_test_scaled if name in ['Logistic Regression', 'Neural Network'] else X_test, y_test)\n",
        "                auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "                # Cross-validation\n",
        "                cv_scores = cross_val_score(model, X_train_scaled if name in ['Logistic Regression', 'Neural Network'] else X_train,\n",
        "                                          y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "                results[name] = {\n",
        "                    'model': model,\n",
        "                    'accuracy': accuracy,\n",
        "                    'auc_score': auc_score,\n",
        "                    'cv_mean': cv_scores.mean(),\n",
        "                    'cv_std': cv_scores.std(),\n",
        "                    'y_pred': y_pred,\n",
        "                    'y_pred_proba': y_pred_proba,\n",
        "                    'classification_report': classification_report(y_test, y_pred)\n",
        "                }\n",
        "\n",
        "                config.logger.info(f\"âœ… {name} trained successfully - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                config.logger.error(f\"âŒ Failed to train {name}: {e}\")\n",
        "\n",
        "        self.results = results\n",
        "        return results\n",
        "\n",
        "    def create_model_comparison(self, results: Dict) -> None:\n",
        "        \"\"\"Create comprehensive model comparison\"\"\"\n",
        "\n",
        "        # Model performance comparison\n",
        "        model_names = list(results.keys())\n",
        "        accuracies = [results[name]['accuracy'] for name in model_names]\n",
        "        auc_scores = [results[name]['auc_score'] for name in model_names]\n",
        "        cv_means = [results[name]['cv_mean'] for name in model_names]\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
        "        fig.suptitle('Comprehensive Model Comparison', fontsize=16)\n",
        "\n",
        "        # Accuracy comparison\n",
        "        bars1 = axes[0,0].bar(model_names, accuracies, color='skyblue', alpha=0.7)\n",
        "        axes[0,0].set_title('Model Accuracy Comparison')\n",
        "        axes[0,0].set_ylabel('Accuracy')\n",
        "        axes[0,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, acc in zip(bars1, accuracies):\n",
        "            axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                          f'{acc:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        # AUC comparison\n",
        "        bars2 = axes[0,1].bar(model_names, auc_scores, color='lightcoral', alpha=0.7)\n",
        "        axes[0,1].set_title('Model AUC Score Comparison')\n",
        "        axes[0,1].set_ylabel('AUC Score')\n",
        "        axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        for bar, auc in zip(bars2, auc_scores):\n",
        "            axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                          f'{auc:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        # Cross-validation scores\n",
        "        cv_stds = [results[name]['cv_std'] for name in model_names]\n",
        "        axes[1,0].errorbar(model_names, cv_means, yerr=cv_stds, fmt='o', capsize=5, capthick=2)\n",
        "        axes[1,0].set_title('Cross-Validation Scores with Standard Deviation')\n",
        "        axes[1,0].set_ylabel('CV Accuracy')\n",
        "        axes[1,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Feature importance (for Random Forest)\n",
        "        if 'Random Forest' in results:\n",
        "            feature_importance = results['Random Forest']['model'].feature_importances_\n",
        "            feature_names = [f'Feature_{i}' for i in range(len(feature_importance))]\n",
        "            sorted_idx = np.argsort(feature_importance)[-10:]  # Top 10 features\n",
        "\n",
        "            axes[1,1].barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n",
        "            axes[1,1].set_yticks(range(len(sorted_idx)))\n",
        "            axes[1,1].set_yticklabels([feature_names[i] for i in sorted_idx])\n",
        "            axes[1,1].set_title('Top 10 Feature Importances (Random Forest)')\n",
        "            axes[1,1].set_xlabel('Importance')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def hyperparameter_tuning(self, X_train, y_train, model_name: str = 'Random Forest'):\n",
        "        \"\"\"Perform hyperparameter tuning\"\"\"\n",
        "\n",
        "        if model_name == 'Random Forest':\n",
        "            model = RandomForestClassifier(random_state=42)\n",
        "            param_grid = {\n",
        "                'n_estimators': [50, 100, 200],\n",
        "                'max_depth': [None, 10, 20],\n",
        "                'min_samples_split': [2, 5, 10],\n",
        "                'min_samples_leaf': [1, 2, 4]\n",
        "            }\n",
        "        elif model_name == 'Gradient Boosting':\n",
        "            model = GradientBoostingClassifier(random_state=42)\n",
        "            param_grid = {\n",
        "                'n_estimators': [50, 100, 200],\n",
        "                'learning_rate': [0.01, 0.1, 0.2],\n",
        "                'max_depth': [3, 5, 7]\n",
        "            }\n",
        "        else:\n",
        "            config.logger.warning(f\"Hyperparameter tuning not implemented for {model_name}\")\n",
        "            return None\n",
        "\n",
        "        grid_search = GridSearchCV(\n",
        "            model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1\n",
        "        )\n",
        "\n",
        "        config.logger.info(f\"Starting hyperparameter tuning for {model_name}...\")\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        config.logger.info(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
        "        config.logger.info(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "        return grid_search.best_estimator_\n",
        "\n",
        "# Initialize ML pipeline\n",
        "ml_pipeline = MLPipeline(llm_manager)\n",
        "\n",
        "# Train models on sample data\n",
        "X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled = ml_pipeline.prepare_data(sample_df, 'promotion')\n",
        "ml_results = ml_pipeline.train_multiple_models(X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled)\n",
        "ml_pipeline.create_model_comparison(ml_results)\n"
      ],
      "metadata": {
        "id": "IqaMMvOKaIez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatabaseManager:\n",
        "    \"\"\"Enhanced database management with multiple database support\"\"\"\n",
        "\n",
        "    def __init__(self, llm_manager: LLMManager):\n",
        "        self.llm_manager = llm_manager\n",
        "        self.engines = {}\n",
        "        self.agents = {}\n",
        "\n",
        "    def create_sample_database(self, df: pd.DataFrame, db_name: str = 'sample_db.sqlite'):\n",
        "        \"\"\"Create sample SQLite database\"\"\"\n",
        "        try:\n",
        "            engine = create_engine(f'sqlite:///{db_name}')\n",
        "            df.to_sql('employee_data', engine, if_exists='replace', index=False)\n",
        "\n",
        "            # Create additional tables for demonstration\n",
        "            departments_df = pd.DataFrame({\n",
        "                'department': ['IT', 'Sales', 'HR', 'Finance', 'Marketing'],\n",
        "                'budget': [500000, 300000, 200000, 400000, 250000],\n",
        "                'head_count': [50, 80, 25, 30, 40]\n",
        "            })\n",
        "            departments_df.to_sql('departments', engine, if_exists='replace', index=False)\n",
        "\n",
        "            self.engines[db_name] = engine\n",
        "            config.logger.info(f\"âœ… Created database: {db_name}\")\n",
        "            return engine\n",
        "\n",
        "        except Exception as e:\n",
        "            config.logger.error(f\"âŒ Failed to create database: {e}\")\n",
        "            return None\n",
        "\n",
        "    def create_sql_agent(self, db_name: str):\n",
        "        \"\"\"Create SQL agent for natural language querying\"\"\"\n",
        "        try:\n",
        "            if db_name not in self.engines:\n",
        "                config.logger.error(f\"Database {db_name} not found\")\n",
        "                return None\n",
        "\n",
        "            db = SQLDatabase(self.engines[db_name])\n",
        "            agent = create_sql_agent(\n",
        "                llm=self.llm_manager.get_model(),\n",
        "                db=db,\n",
        "                agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "                verbose=True,\n",
        "                handle_parsing_errors=True,\n",
        "                max_iterations=5\n",
        "            )\n",
        "\n",
        "            self.agents[db_name] = agent\n",
        "            config.logger.info(f\"âœ… Created SQL agent for {db_name}\")\n",
        "            return agent\n",
        "\n",
        "        except Exception as e:\n",
        "            config.logger.error(f\"âŒ Failed to create SQL agent: {e}\")\n",
        "            return None\n",
        "\n",
        "    def execute_natural_language_query(self, db_name: str, query: str):\n",
        "        \"\"\"Execute natural language database query\"\"\"\n",
        "        try:\n",
        "            if db_name not in self.agents:\n",
        "                config.logger.error(f\"SQL agent for {db_name} not found\")\n",
        "                return None\n",
        "\n",
        "            agent = self.agents[db_name]\n",
        "            result = agent.invoke(query)\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            config.logger.error(f\"âŒ Query execution failed: {e}\")\n",
        "            return None\n",
        "\n",
        "# Initialize database manager\n",
        "db_manager = DatabaseManager(llm_manager)\n",
        "\n",
        "# Create sample database\n",
        "engine = db_manager.create_sample_database(sample_df)\n",
        "sql_agent = db_manager.create_sql_agent('sample_db.sqlite')\n",
        "\n",
        "# Example queries\n",
        "sample_queries = [\n",
        "    \"How many employees are in each department?\",\n",
        "    \"What's the average performance score by department?\",\n",
        "    \"Show me the top 10 employees by performance score\",\n",
        "    \"What's the correlation between education years and income?\",\n",
        "    \"Which department has the highest promotion rate?\"\n",
        "]\n",
        "\n",
        "print(\"Sample Natural Language Queries:\")\n",
        "for i, query in enumerate(sample_queries, 1):\n",
        "    print(f\"{i}. {query}\")\n"
      ],
      "metadata": {
        "id": "NZXNxyHDjmIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGSystem:\n",
        "    \"\"\"Enhanced Retrieval-Augmented Generation system\"\"\"\n",
        "\n",
        "    def __init__(self, llm_manager: LLMManager):\n",
        "        self.llm_manager = llm_manager\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.vector_stores = {}\n",
        "        self.qa_chains = {}\n",
        "\n",
        "    def create_document_store(self, documents: List[str], store_name: str = 'default'):\n",
        "        \"\"\"Create vector store from documents\"\"\"\n",
        "        try:\n",
        "            # Split documents into chunks\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=1000,\n",
        "                chunk_overlap=200,\n",
        "                length_function=len\n",
        "            )\n",
        "\n",
        "            # Handle both string documents and document objects\n",
        "            if isinstance(documents[0], str):\n",
        "                splits = text_splitter.split_text('\\n\\n'.join(documents))\n",
        "            else:\n",
        "                texts = text_splitter.split_documents(documents)\n",
        "                splits = [doc.page_content for doc in texts]\n",
        "\n",
        "            # Create vector store\n",
        "            vector_store = FAISS.from_texts(splits, self.embeddings)\n",
        "            self.vector_stores[store_name] = vector_store\n",
        "\n",
        "            config.logger.info(f\"âœ… Created vector store '{store_name}' with {len(splits)} chunks\")\n",
        "            return vector_store\n",
        "\n",
        "        except Exception as e:\n",
        "            config.logger.error(f\"âŒ Failed to create vector store: {e}\")\n",
        "            return None\n",
        "\n",
        "    def create_qa_chain(self, store_name: str = 'default'):\n",
        "        \"\"\"Create QA chain for the vector store\"\"\"\n",
        "        try:\n",
        "            if store_name not in self.vector_stores:\n",
        "                config.logger.error(f\"Vector store '{store_name}' not found\")\n",
        "                return None\n",
        "\n",
        "            vector_store = self.vector_stores[store_name]\n",
        "\n",
        "            # Custom prompt template\n",
        "            prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "            PROMPT = PromptTemplate(\n",
        "                template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "            )\n",
        "\n",
        "            qa_chain = RetrievalQA.from_chain_type(\n",
        "                llm=self.llm_manager.get_model(),\n",
        "                chain_type=\"stuff\",\n",
        "                retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "                chain_type_kwargs={\"prompt\": PROMPT},\n",
        "                return_source_documents=True\n",
        "            )\n",
        "\n",
        "            self.qa_chains[store_name] = qa_chain\n",
        "            config.logger.info(f\"âœ… Created QA chain for '{store_name}'\")\n",
        "            return qa_chain\n",
        "\n",
        "        except Exception as e:\n",
        "            config.logger.error(f\"âŒ Failed to create QA chain: {e}\")\n",
        "            return None\n",
        "\n",
        "    def query_documents(self, question: str, store_name: str = 'default'):\n",
        "        \"\"\"Query the document store\"\"\"\n",
        "        try:\n",
        "            if store_name not in self.qa_chains:\n",
        "                config.logger.error(f\"QA chain for '{store_name}' not found\")\n",
        "                return None\n",
        "\n",
        "            qa_chain = self.qa_chains[store_name]\n",
        "            result = qa_chain.invoke({\"query\": question})\n",
        "\n",
        "            return {\n",
        "                'answer': result['result'],\n",
        "                'source_documents': result['source_documents']\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            config.logger.error(f\"âŒ Query failed: {e}\")\n",
        "            return None\n",
        "\n",
        "# Initialize RAG system\n",
        "rag_system = RAGSystem(llm_manager)\n",
        "\n",
        "# Create sample documents for demonstration\n",
        "sample_documents = [\n",
        "    \"\"\"\n",
        "    Data Science Best Practices:\n",
        "    1. Always start with exploratory data analysis (EDA)\n",
        "    2. Handle missing values appropriately\n",
        "    3. Feature engineering is crucial for model performance\n",
        "    4. Cross-validation prevents overfitting\n",
        "    5. Document your methodology and assumptions\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Machine Learning Model Selection Guidelines:\n",
        "    - Use logistic regression for simple binary classification\n",
        "    - Random Forest is good for handling mixed data types\n",
        "    - Gradient boosting often provides high accuracy\n",
        "    - Neural networks work well with large datasets\n",
        "    - Always compare multiple models using cross-validation\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Data Visualization Principles:\n",
        "    - Choose the right chart type for your data\n",
        "    - Use color purposefully and consistently\n",
        "    - Avoid chart junk and unnecessary elements\n",
        "    - Make sure text is readable\n",
        "    - Include proper labels and legends\n",
        "    - Consider your audience when designing visualizations\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "# Create document store and QA chain\n",
        "vector_store = rag_system.create_document_store(sample_documents, 'data_science_guide')\n",
        "qa_chain = rag_system.create_qa_chain('data_science_guide')\n",
        "\n",
        "# Example RAG queries\n",
        "rag_queries = [\n",
        "    \"What are the key steps in data science projects?\",\n",
        "    \"How should I select a machine learning model?\",\n",
        "    \"What are important principles for data visualization?\",\n",
        "    \"How can I prevent overfitting in my models?\"\n",
        "]\n",
        "\n",
        "print(\"Sample RAG Queries:\")\n",
        "for i, query in enumerate(rag_queries, 1):\n",
        "    print(f\"{i}. {query}\")\n"
      ],
      "metadata": {
        "id": "uLDRacTcjmQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestSuite:\n",
        "    \"\"\"Comprehensive testing suite for the data science assistant\"\"\"\n",
        "\n",
        "    def __init__(self, data_processor, ml_pipeline, db_manager, rag_system):\n",
        "        self.data_processor = data_processor\n",
        "        self.ml_pipeline = ml_pipeline\n",
        "        self.db_manager = db_manager\n",
        "        self.rag_system = rag_system\n",
        "        self.test_results = {}\n",
        "\n",
        "    def test_data_processing(self):\n",
        "        \"\"\"Test data processing functionality\"\"\"\n",
        "        tests = {}\n",
        "\n",
        "        # Test data loading\n",
        "        try:\n",
        "            test_df = self.data_processor.data.get('employee_data')\n",
        "            tests['data_loading'] = test_df is not None and not test_df.empty\n",
        "        except:\n",
        "            tests['data_loading'] = False\n",
        "\n",
        "        # Test agent creation\n",
        "        try:\n",
        "            agent = self.data_processor.agents.get('employee_data')\n",
        "            tests['agent_creation'] = agent is not None\n",
        "        except:\n",
        "            tests['agent_creation'] = False\n",
        "\n",
        "        # Test data analysis\n",
        "        try:\n",
        "            analysis = self.data_processor.analyze_dataset(test_df)\n",
        "            tests['data_analysis'] = 'shape' in analysis and 'columns' in analysis\n",
        "        except:\n",
        "            tests['data_analysis'] = False\n",
        "\n",
        "        self.test_results['data_processing'] = tests\n",
        "        return tests\n",
        "\n",
        "    def test_ml_pipeline(self):\n",
        "        \"\"\"Test machine learning pipeline\"\"\"\n",
        "        tests = {}\n",
        "\n",
        "        # Test model training\n",
        "        try:\n",
        "            tests['model_training'] = len(self.ml_pipeline.results) > 0\n",
        "        except:\n",
        "            tests['model_training'] = False\n",
        "\n",
        "        # Test model accuracy\n",
        "        try:\n",
        "            accuracies = [result['accuracy'] for result in self.ml_pipeline.results.values()]\n",
        "            tests['model_accuracy'] = all(acc > 0.5 for acc in accuracies)\n",
        "        except:\n",
        "            tests['model_accuracy'] = False\n",
        "\n",
        "        self.test_results['ml_pipeline'] = tests\n",
        "        return tests\n",
        "\n",
        "    def test_database_integration(self):\n",
        "        \"\"\"Test database integration\"\"\"\n",
        "        tests = {}\n",
        "\n",
        "        # Test database creation\n",
        "        try:\n",
        "            tests['database_creation'] = 'sample_db.sqlite' in self.db_manager.engines\n",
        "        except:\n",
        "            tests['database_creation'] = False\n",
        "\n",
        "        # Test SQL agent\n",
        "        try:\n",
        "            tests['sql_agent'] = 'sample_db.sqlite' in self.db_manager.agents\n",
        "        except:\n",
        "            tests['sql_agent'] = False\n",
        "\n",
        "        self.test_results['database'] = tests\n",
        "        return tests\n",
        "\n",
        "    def test_rag_system(self):\n",
        "        \"\"\"Test RAG system\"\"\"\n",
        "        tests = {}\n",
        "\n",
        "        # Test vector store creation\n",
        "        try:\n",
        "            tests['vector_store'] = 'data_science_guide' in self.rag_system.vector_stores\n",
        "        except:\n",
        "            tests['vector_store'] = False\n",
        "\n",
        "        # Test QA chain\n",
        "        try:\n",
        "            tests['qa_chain'] = 'data_science_guide' in self.rag_system.qa_chains\n",
        "        except:\n",
        "            tests['qa_chain'] = False\n",
        "\n",
        "        # Test query functionality\n",
        "        try:\n",
        "            result = self.rag_system.query_documents(\"What is data science?\", 'data_science_guide')\n",
        "            tests['query_functionality'] = result is not None and 'answer' in result\n",
        "        except:\n",
        "            tests['query_functionality'] = False\n",
        "\n",
        "        self.test_results['rag_system'] = tests\n",
        "        return tests\n",
        "\n",
        "    def run_all_tests(self):\n",
        "        \"\"\"Run all tests and generate report\"\"\"\n",
        "        print(\"ðŸ§ª Running Comprehensive Test Suite...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Run individual test suites\n",
        "        data_tests = self.test_data_processing()\n",
        "        ml_tests = self.test_ml_pipeline()\n",
        "        db_tests = self.test_database_integration()\n",
        "        rag_tests = self.test_rag_system()\n",
        "\n",
        "        # Generate test report\n",
        "        all_tests = {\n",
        "            'Data Processing': data_tests,\n",
        "            'ML Pipeline': ml_tests,\n",
        "            'Database Integration': db_tests,\n",
        "            'RAG System': rag_tests\n",
        "        }\n",
        "\n",
        "        print(\"\\nðŸ“Š Test Results Summary:\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        total_tests = 0\n",
        "        passed_tests = 0\n",
        "\n",
        "        for category, tests in all_tests.items():\n",
        "            category_passed = sum(tests.values())\n",
        "            category_total = len(tests)\n",
        "            total_tests += category_total\n",
        "            passed_tests += category_passed\n",
        "\n",
        "            status = \"âœ…\" if category_passed == category_total else \"âš ï¸\"\n",
        "            print(f\"{status} {category}: {category_passed}/{category_total} tests passed\")\n",
        "\n",
        "            for test_name, result in tests.items():\n",
        "                status_icon = \"âœ…\" if result else \"âŒ\"\n",
        "                print(f\"   {status_icon} {test_name}\")\n",
        "\n",
        "        print(\"-\" * 30)\n",
        "        overall_status = \"âœ…\" if passed_tests == total_tests else \"âš ï¸\"\n",
        "        print(f\"{overall_status} Overall: {passed_tests}/{total_tests} tests passed\")\n",
        "\n",
        "        if passed_tests == total_tests:\n",
        "            print(\"\\nðŸŽ‰ All tests passed! System is ready for use.\")\n",
        "        else:\n",
        "            print(f\"\\nâš ï¸ {total_tests - passed_tests} test(s) failed. Please check the issues above.\")\n",
        "\n",
        "        return all_tests\n",
        "\n",
        "# Initialize and run test suite\n",
        "test_suite = TestSuite(data_processor, ml_pipeline, db_manager, rag_system)\n",
        "test_results = test_suite.run_all_tests()\n"
      ],
      "metadata": {
        "id": "wlfflyKjjmUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InteractiveDemo:\n",
        "    \"\"\"Interactive demonstration interface\"\"\"\n",
        "\n",
        "    def __init__(self, data_processor, ml_pipeline, db_manager, rag_system, visualizer):\n",
        "        self.data_processor = data_processor\n",
        "        self.ml_pipeline = ml_pipeline\n",
        "        self.db_manager = db_manager\n",
        "        self.rag_system = rag_system\n",
        "        self.visualizer = visualizer\n",
        "\n",
        "    def run_interactive_demo(self):\n",
        "        \"\"\"Run interactive demonstration\"\"\"\n",
        "        print(\"ðŸš€ Welcome to the Enhanced LangChain Data Science Assistant!\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        while True:\n",
        "            print(\"\\nAvailable Demos:\")\n",
        "            print(\"1. ðŸ“Š Data Analysis with Natural Language\")\n",
        "            print(\"2. ðŸ¤– Machine Learning Pipeline\")\n",
        "            print(\"3. ðŸ—„ï¸ Natural Language Database Queries\")\n",
        "            print(\"4. ðŸ“š RAG Document Q&A\")\n",
        "            print(\"5. ðŸ“ˆ Advanced Visualizations\")\n",
        "            print(\"6. ðŸ§ª Run System Tests\")\n",
        "            print(\"0. Exit\")\n",
        "\n",
        "            choice = input(\"\\nEnter your choice (0-6): \").strip()\n",
        "\n",
        "            if choice == '0':\n",
        "                print(\"ðŸ‘‹ Thank you for using the Data Science Assistant!\")\n",
        "                break\n",
        "            elif choice == '1':\n",
        "                self.demo_data_analysis()\n",
        "            elif choice == '2':\n",
        "                self.demo_ml_pipeline()\n",
        "            elif choice == '3':\n",
        "                self.demo_database_queries()\n",
        "            elif choice == '4':\n",
        "                self.demo_rag_system()\n",
        "            elif choice == '5':\n",
        "                self.demo_visualizations()\n",
        "            elif choice == '6':\n",
        "                test_suite.run_all_tests()\n",
        "            else:\n",
        "                print(\"âŒ Invalid choice. Please try again.\")\n",
        "\n",
        "    def demo_data_analysis(self):\n",
        "        \"\"\"Demonstrate data analysis capabilities\"\"\"\n",
        "        print(\"\\nðŸ“Š Data Analysis Demo\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        sample_questions = [\n",
        "            \"How many rows and columns are in the dataset?\",\n",
        "            \"What is the average age of employees?\",\n",
        "            \"Show me the correlation between performance score and promotion\",\n",
        "            \"What percentage of employees got promoted?\",\n",
        "            \"Create a summary of the dataset\"\n",
        "        ]\n",
        "\n",
        "        print(\"Sample questions you can ask:\")\n",
        "        for i, q in enumerate(sample_questions, 1):\n",
        "            print(f\"{i}. {q}\")\n",
        "\n",
        "        question = input(\"\\nEnter your question (or press Enter for a sample): \").strip()\n",
        "        if not question:\n",
        "            question = sample_questions[0]\n",
        "\n",
        "        try:\n",
        "            agent = self.data_processor.agents.get('employee_data')\n",
        "            if agent:\n",
        "                print(f\"\\nðŸ¤– Processing: {question}\")\n",
        "                response = agent.invoke(question)\n",
        "                print(f\"ðŸ“‹ Answer: {response}\")\n",
        "            else:\n",
        "                print(\"âŒ Data analysis agent not available\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error: {e}\")\n",
        "\n",
        "    def demo_ml_pipeline(self):\n",
        "        \"\"\"Demonstrate ML pipeline\"\"\"\n",
        "        print(\"\\nðŸ¤– Machine Learning Pipeline Demo\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        if self.ml_pipeline.results:\n",
        "            print(\"ðŸ“ˆ Model Performance Summary:\")\n",
        "            for model_name, results in self.ml_pipeline.results.items():\n",
        "                print(f\"  â€¢ {model_name}: {results['accuracy']:.3f} accuracy, {results['auc_score']:.3f} AUC\")\n",
        "        else:\n",
        "            print(\"âŒ ML models not trained yet\")\n",
        "\n",
        "    def demo_database_queries(self):\n",
        "        \"\"\"Demonstrate database querying\"\"\"\n",
        "        print(\"\\nðŸ—„ï¸ Natural Language Database Queries Demo\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        if 'sample_db.sqlite' in self.db_manager.agents:\n",
        "            question = input(\"Enter your database question: \").strip()\n",
        "            if question:\n",
        "                try:\n",
        "                    result = self.db_manager.execute_natural_language_query('sample_db.sqlite', question)\n",
        "                    print(f\"ðŸ“‹ Result: {result}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"âŒ Error: {e}\")\n",
        "        else:\n",
        "            print(\"âŒ Database agent not available\")\n",
        "\n",
        "    def demo_rag_system(self):\n",
        "        \"\"\"Demonstrate RAG system\"\"\"\n",
        "        print(\"\\nðŸ“š RAG Document Q&A Demo\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        if 'data_science_guide' in self.rag_system.qa_chains:\n",
        "            question = input(\"Ask a question about data science: \").strip()\n",
        "            if question:\n",
        "                try:\n",
        "                    result = self.rag_system.query_documents(question, 'data_science_guide')\n",
        "                    if result:\n",
        "                        print(f\"ðŸ“‹ Answer: {result['answer']}\")\n",
        "                        print(f\"ðŸ“„ Sources: {len(result['source_documents'])} documents used\")\n",
        "                except Exception as e:\n",
        "                    print(f\"âŒ Error: {e}\")\n",
        "        else:\n",
        "            print(\"âŒ RAG system not available\")\n",
        "\n",
        "    def demo_visualizations(self):\n",
        "        \"\"\"Demonstrate visualization capabilities\"\"\"\n",
        "        print(\"\\nðŸ“ˆ Advanced Visualizations Demo\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        try:\n",
        "            self.visualizer.create_comprehensive_eda(sample_df, 'promotion')\n",
        "            print(\"âœ… Visualization created successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Visualization error: {e}\")\n",
        "\n",
        "# Initialize interactive demo\n",
        "demo = InteractiveDemo(data_processor, ml_pipeline, db_manager, rag_system, visualizer)\n"
      ],
      "metadata": {
        "id": "E-DNnBCvjmWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProductionUtils:\n",
        "    \"\"\"Production deployment utilities\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def create_streamlit_app():\n",
        "        \"\"\"Create Streamlit web application\"\"\"\n",
        "        app_code = '''\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from your_enhanced_data_science_assistant import *\n",
        "\n",
        "st.set_page_config(page_title=\"Data Science Assistant\", layout=\"wide\")\n",
        "\n",
        "st.title(\"ðŸ¤– Enhanced LangChain Data Science Assistant\")\n",
        "\n",
        "# Sidebar navigation\n",
        "st.sidebar.title(\"Navigation\")\n",
        "page = st.sidebar.selectbox(\"Choose a feature\", [\n",
        "    \"Data Analysis\", \"Machine Learning\", \"Database Queries\", \"RAG Q&A\", \"Visualizations\"\n",
        "])\n",
        "\n",
        "if page == \"Data Analysis\":\n",
        "    st.header(\"ðŸ“Š Data Analysis\")\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Upload CSV file\", type=\"csv\")\n",
        "    if uploaded_file:\n",
        "        df = pd.read_csv(uploaded_file)\n",
        "        st.write(\"Dataset Preview:\")\n",
        "        st.dataframe(df.head())\n",
        "\n",
        "        question = st.text_input(\"Ask a question about your data:\")\n",
        "        if question and st.button(\"Analyze\"):\n",
        "            # Process with your data agent\n",
        "            st.write(\"Analysis results would appear here\")\n",
        "\n",
        "elif page == \"Machine Learning\":\n",
        "    st.header(\"ðŸ¤– Machine Learning Pipeline\")\n",
        "    # ML interface code here\n",
        "\n",
        "elif page == \"Database Queries\":\n",
        "    st.header(\"ðŸ—„ï¸ Natural Language Database Queries\")\n",
        "    # Database interface code here\n",
        "\n",
        "elif page == \"RAG Q&A\":\n",
        "    st.header(\"ðŸ“š Document Q&A\")\n",
        "    # RAG interface code here\n",
        "\n",
        "elif page == \"Visualizations\":\n",
        "    st.header(\"ðŸ“ˆ Advanced Visualizations\")\n",
        "    # Visualization interface code here\n",
        "'''\n",
        "\n",
        "        with open('streamlit_app.py', 'w') as f:\n",
        "            f.write(app_code)\n",
        "\n",
        "        print(\"âœ… Streamlit app created: streamlit_app.py\")\n",
        "        print(\"Run with: streamlit run streamlit_app.py\")\n",
        "\n",
        "    @staticmethod\n",
        "    def create_docker_config():\n",
        "        \"\"\"Create Docker configuration\"\"\"\n",
        "        dockerfile = '''\n",
        "FROM python:3.9-slim\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "COPY requirements.txt .\n",
        "RUN pip install -r requirements.txt\n",
        "\n",
        "COPY . .\n",
        "\n",
        "EXPOSE 8501\n",
        "\n",
        "CMD [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]\n",
        "'''\n",
        "\n",
        "        requirements = '''\n",
        "langchain>=0.1.0\n",
        "langchain-openai>=0.1.0\n",
        "langchain-experimental>=0.0.50\n",
        "streamlit>=1.25.0\n",
        "pandas>=2.0.0\n",
        "plotly>=5.15.0\n",
        "scikit-learn>=1.3.0\n",
        "'''\n",
        "\n",
        "        with open('Dockerfile', 'w') as f:\n",
        "            f.write(dockerfile)\n",
        "\n",
        "        with open('requirements.txt', 'w') as f:\n",
        "            f.write(requirements)\n",
        "\n",
        "        print(\"âœ… Docker configuration created\")\n",
        "        print(\"Build with: docker build -t data-science-assistant .\")\n",
        "        print(\"Run with: docker run -p 8501:8501 data-science-assistant\")\n",
        "\n",
        "    @staticmethod\n",
        "    def save_models_and_data(ml_pipeline, data_processor):\n",
        "        \"\"\"Save trained models and processed data\"\"\"\n",
        "        import pickle\n",
        "\n",
        "        # Save models\n",
        "        with open('trained_models.pkl', 'wb') as f:\n",
        "            pickle.dump(ml_pipeline.results, f)\n",
        "\n",
        "        # Save processed data\n",
        "        with open('processed_data.pkl', 'wb') as f:\n",
        "            pickle.dump(data_processor.data, f)\n",
        "\n",
        "        print(\"âœ… Models and data saved successfully\")\n",
        "\n",
        "# Create production utilities\n",
        "prod_utils = ProductionUtils()\n"
      ],
      "metadata": {
        "id": "E0t70jIhjmZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"ðŸš€ Enhanced LangChain Data Science Assistant\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Check if all components are initialized\n",
        "    components_status = {\n",
        "        \"Configuration\": config is not None,\n",
        "        \"LLM Manager\": llm_manager is not None,\n",
        "        \"Data Processor\": data_processor is not None,\n",
        "        \"Visualizer\": visualizer is not None,\n",
        "        \"ML Pipeline\": ml_pipeline is not None,\n",
        "        \"Database Manager\": db_manager is not None,\n",
        "        \"RAG System\": rag_system is not None\n",
        "    }\n",
        "\n",
        "    print(\"\\nðŸ“‹ Component Status:\")\n",
        "    for component, status in components_status.items():\n",
        "        status_icon = \"âœ…\" if status else \"âŒ\"\n",
        "        print(f\"{status_icon} {component}\")\n",
        "\n",
        "    if all(components_status.values()):\n",
        "        print(\"\\nðŸŽ‰ All components initialized successfully!\")\n",
        "        print(\"\\nAvailable features:\")\n",
        "        print(\"â€¢ ðŸ“Š Automated EDA and data analysis\")\n",
        "        print(\"â€¢ ðŸ¤– Multiple ML model training and comparison\")\n",
        "        print(\"â€¢ ðŸ“ˆ Advanced interactive visualizations\")\n",
        "        print(\"â€¢ ðŸ—„ï¸ Natural language database querying\")\n",
        "        print(\"â€¢ ðŸ“š RAG-based document Q&A\")\n",
        "        print(\"â€¢ ðŸ§ª Comprehensive testing suite\")\n",
        "        print(\"â€¢ ðŸš€ Production deployment utilities\")\n",
        "\n",
        "        # Option to run interactive demo\n",
        "        run_demo = input(\"\\nWould you like to run the interactive demo? (y/n): \").lower().strip()\n",
        "        if run_demo == 'y':\n",
        "            demo.run_interactive_demo()\n",
        "        else:\n",
        "            print(\"\\nðŸ’¡ You can run demo.run_interactive_demo() anytime to explore features!\")\n",
        "\n",
        "        # Option to create production files\n",
        "        create_prod = input(\"\\nWould you like to create production deployment files? (y/n): \").lower().strip()\n",
        "        if create_prod == 'y':\n",
        "            prod_utils.create_streamlit_app()\n",
        "            prod_utils.create_docker_config()\n",
        "            prod_utils.save_models_and_data(ml_pipeline, data_processor)\n",
        "\n",
        "    else:\n",
        "        print(\"\\nâš ï¸ Some components failed to initialize. Please check the errors above.\")\n",
        "\n",
        "# Execute main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "TVSs67ezjmcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z4pCpatkjmfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SLp-su0Zjmh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wsIJOUlvjmlL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}